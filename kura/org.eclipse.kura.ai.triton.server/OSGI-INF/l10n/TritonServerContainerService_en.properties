name=Nvidia Triton Docker Container Server
description=Configuration for the Local Container Nvidia Triton Server
containerImage=Image name
containerImageDesc=The image the container will be created with.
containerImageTag=Image tag
containerImageTagDesc=Describes which image version that should be used for creating the container.
serverPorts=Nvidia Triton Server ports
serverPortsDesc=The ports used to connect to the server for HTTP, GPRC and Metrics services.
localModelRepositoryPath=Local model repository path
localModelRepositoryPathDesc=Specify the path on the filesystem where the models are stored.
localModelPassword=Local model decryption password
localModelPasswordDesc=Specify the password to be used for decrypting models stored in the model repository. If none is specified, models are supposed to be plaintext.
models=Inference Models
modelsDesc=A comma separated list of inference model names that the server will load.
localBackendsPath=Local backends path
localBackendsPathDesc=Specify the host filesystem path where the backends are stored. If left blank, the backends provided by the Triton container will be used.
localBackendsConfig=Optional configuration for the local backends
localBackendsConfigDesc=A semi-colon separated list of configuration for the backends. i.e. tensorflow,version=2;tensorflow,allow-soft-placement=false
containerMemory=Memory
containerMemoryDesc=The maximum amount of memory the container can use in bytes. Set it as a positive integer, optionally followed by a suffix of b, k, m, g, to indicate bytes, kilobytes, megabytes, or gigabytes.\nThe minimum allowed value is platform dependent (i.e. 6m). If left empty, the memory assigned to the container will be set to a default value by the native container orchestrator.
containerCpus=CPUs
containerCpusDesc=Specify how many CPUs the Triton container can use. Decimal values are allowed, so if set to 1.5, the container will use at most one and a half cpu resource.
containerGpus=GPUs
containerGpusDesc=Specify how many Nvidia GPUs the Triton container can use. Allowed values are 'all' or an integer number. If there's no Nvidia GPU installed, leave the field empty. If the Nvidia Container Runtime is used, leave the field empty.
containerRuntime=Runtime
containerRuntimeDesc=Specify the runtime the Triton container can use. If the container uses the Nvidia Container Runtime, set it to 'nvidia' and leave the GPUs field empty.
devices=Devices
devicesDesc=Specifies, as a comma separated list of device paths, the host devices that the Triton container can access.
timeout=Timeout (in seconds) for time consuming tasks
timeoutDesc=Timeout (in seconds) for time consuming tasks like server startup, shutdown or model load. If the task exceeds the timeout, the operation will be terminated with an error.
grpcMaxSize=Max. GRPC message size (bytes)
grpcMaxSizeDesc=Maximum accepted input size for the GRPC calls.\nIncrease this value if the model input size is bigger than the default.