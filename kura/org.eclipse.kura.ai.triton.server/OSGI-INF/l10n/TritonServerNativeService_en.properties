name=Local Nvidia Triton Server
description=Configuration for the Local Native Nvidia Triton Server
serverPorts=Nvidia Triton Server ports
serverPortsDesc=The ports used to connect to the server for HTTP, GPRC and Metrics services.
localModelRepositoryPath=Local model repository path
localModelRepositoryPathDesc=Specify the path on the filesystem where the models are stored.
localModelRepositoryPassword=Local model decryption password
localModelRepositoryPasswordDesc=Specify the password to be used for decrypting models stored in the model repository. If none is specified, models are supposed to be plaintext.
models=Inference Models
modelsDesc=A comma separated list of inference model names that the server will load.
localBackendsPath=Local backends path
localBackendsPathDesc=Specify the path on the filesystem where the backends are stored.
localBackendsConfig=Optional configuration for the local backends
localBackendsConfigDesc=A semi-colon separated list of configuration for the backends. i.e. tensorflow,version=2;tensorflow,allow-soft-placement=false
timeout=Timeout (in seconds) for time consuming tasks
timeoutDesc=Timeout (in seconds) for time consuming tasks like server startup, shutdown or model load. If the task exceeds the timeout, the operation will be terminated with an error.
grpcMaxSize=Max. GRPC message size (bytes)
grpcMaxSizeDesc=Maximum accepted input size for the GRPC calls.Increase this value if the model input size is bigger than the default.