name=Nvidia Triton Server
description=Configuration for the Nvidia Triton Server
enableLocal=Local Nvidia Triton Server
enableLocalDesc=If enabled, a local native Nvidia Triton Server is started. In this case, the model repository and backends path are mandatory. Moreover, the server address property is overridden and set to localhost.
serverAddress=Nvidia Triton Server address
serverAddressDesc=The address of the Nvidia Triton Server.
serverPorts=Nvidia Triton Server ports
serverPortsDesc=The ports used to connect to the server for HTTP, GPRC and Metrics services.
models=Inference Models
modelsDesc=A comma separated list of inference model names that the server will load.
localModelPassword=Local model decryption password
localModelPasswordDesc=Only for local instance, specify the password to be used for decrypting models stored in the model repository. If none is specified, models are supposed to be plaintext.
localModelRepositoryPath=Local model repository path
localModelRepositoryPathDesc=Only for local instance, specify the path on the filesystem where the models are stored.
localBackendsPath=Local backends path
localBackendsPathDesc=Only for local instance, specify the path on the filesystem where the backends are stored.
localBackendsConfig=Optional configuration for the local backends
localBackendsConfigDesc=Only for local instance, a semi-colon separated list of configuration for the backends. i.e. tensorflow,version=2;tensorflow,allow-soft-placement=false